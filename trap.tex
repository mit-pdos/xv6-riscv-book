%    Sidebar about panic:
% 	panic is the kernel's last resort: the impossible has happened and the
% 	kernel does not know how to proceed.  In xv6, panic does ...
\chapter{Traps, interrupts, and drivers}
\label{CH:TRAP}

When running a process, a CPU executes the normal processing loop: read an
instruction, advance the program counter, execute the instruction, repeat.  But
there are events on which control from a user program must transfer back to the
kernel instead of executing the next instruction.  These events include a device
signaling that it wants attention, a user program doing something illegal (e.g.,
references a virtual address for which there is no page table entry), or a user
program asking the kernel for a service with a system call.  There are three
main challenges in handling these events: 1) the kernel must arrange that a
CPU switches from user mode to kernel mode (and back); 2) the kernel and
devices must coordinate their parallel activities; and 3) the kernel must
understand the interface of the devices.  Addressing these 3 challenges requires
detailed understanding of hardware and careful programming, and can result in
opaque kernel code.  This chapter explains how xv6 addresses these three
challenges.

\section{Systems calls, exceptions, and interrupts}

There are three cases when control must be transferred from a user program to
the kernel. First, a system call: when a user program asks for an operating
system service, as we saw at the end of the last chapter.
Second, an
\indextext{exception}:
when a program performs an illegal action. Examples of illegal actions include
divide by zero, attempt to access memory for a page-table entry that is not
present, and so on.  Third, an
\indextext{interrupt}:
when a device generates a signal to indicate that
it needs attention from the operating system.  For example, a clock chip may
generate an interrupt every 10 ms to allow the kernel to implement
time sharing.  As another example, when the disk has read a block from
disk, it generates an interrupt to alert the operating system that the
block is ready to be retrieved.

The kernel handles all interrupts, rather than processes
handling them, because in most cases only the kernel has the
required privilege and state. For example, in order to time-slice
among processes in response to timer interrupts, the kernel
must be involved, if only to force uncooperative processes to
yield the CPU.

In all three cases, the operating system design must arrange for the
following to happen.  The system must save the CPU's registers for future
transparent resume.  The system must be set up for execution
in the kernel.  The system must chose a place for the kernel to start
executing. The kernel must be able to retrieve information about the
event, e.g., system call arguments.  It must all be done securely; the system
must maintain isolation of user processes and the kernel.

A word on terminology: this chapter uses the term \indextext{trap} to refer
to exceptions and systems calls.  It also uses
traps and interrupts
interchangeably, but it is important to remember that traps are caused
by the current process running on a CPU (e.g., the process makes
a system call and as a result generates a trap), and interrupts are
caused by devices and may not be related to the currently running
process.  For example, a disk may generate an interrupt when it is
done retrieving a block for one process, but at the time of the
interrupt some other process may be running.  This property of
interrupts makes thinking about interrupts more difficult than
thinking about traps, because interrupts happen concurrently with
other activities.

\section{RISC-V Interrupts}

Devices on the RISC-V development board can generate interrupts, and
xv6 must set up the hardware to handle these interrupts.  Devices
usually interrupt in order to tell the kernel that some hardware event
has occured, such as I/O completion.  Interrupts are usually optional
in the sense that the kernel could instead periodically check (or
\indextext{poll}) the device hardware to check for new
events.  Interrupts are preferable to polling if the events are
relatively rare, so that polling would waste CPU time.

Devices can generate interrupts
at any time.  There is hardware on the board to signal the CPU
when a device needs attention (e.g., the user has typed a character on
the keyboard). We must program the device to generate an interrupt, and
arrange that a CPU receives the interrupt. 

Let's look at the timer device and timer interrupts.  We would like
the timer hardware to generate an interrupt, say, 100 times per
second so that the kernel can track the passage of time and so the
kernel can time-slice among multiple running processes.  The choice of
100 times per second allows for decent interactive performance while
not swamping the CPU with handling interrupts.

RISC-V on which xv6 is running has a \textit{Core Local
  Interruptor (CLINT)}\index{Core Local Interruptor (CLINT)}~\cite{u54}, which
can be programmed to generate timer interrupts.  The CLINT is a
memory-mapped device located at \lstinline{0x02000000} (see
Fig.~\ref{fig:xv6_layout}), and can thus be programmed with ordinary
load and store instructions.  xv6 programs the CLINT during start
\lineref{kernel/start.c:/CLINT/}: it reads the clock from address
\lstinline{CLINT_MTIME}, adds 10,000 cycles to it, and stores that
value in this core's \lstinline{CLINT_MTIMECMP(id)} timer register.
The hardware clock runs at 1Mhz and after 10,000 cycles (i.e., 10ms)
the clock value will be as large as the value in
\lstinline{CLINT_MTIMECMP}, and the CLINT will interrupt core
\lstinline{id}.

\section{Entering and exiting the kernel}

RISC-V has minimal mechanism for handling interrupts,
and much of the work of entering and leaving the kernel is done by the kernel
itself.   RISC-V has registers (\indexcode{mtvec} and
\indexcode{stvec}) to specify the address of the \indextext{interrupt
  handler} in machine mode or supervisor mode, respectively.  It also
has registers (\indexcode{mip} and \indexcode{sip}) to specify which
interrupts are taken in machine mode and in supervisor mode,
respectively.  When a devices raises an interrupt, the CPU sees
whether the interrupt must be handled in machine mode or supervisor
mode by inspecting the \lstinline{ip} registers, switches to that
mode, changes the program counter to the value in the mode's
\lstinline{tvec} register, saves the value of the program counter at
the time of interrupt in \indexcode{mepc} or in \indexcode{sepc}, and
saves the previous mode in \indexcode{mstatus} or \indexcode{sstatus}.

It is important that the CPU performs all these steps in a
single operation.  Consider if one of these steps were omitted: for
example, the CPU didn't switch program counters.  Then, an
interrupt may cause a switch from user mode to kernel mode with an
entry point chosen by the user (i.e., namely the value of the
user-controlled program counter).  This would allow the user program
to run any kernel code of its choice, which allows the user program to
break the isolation between user and kernel. For example, the user
program can run the code to switch to the kernel page table, because
it is in kernel mode it can write the \lstinline{satp} register.  It
is thus important that the kernel specify the entry point, and not the
user program.

Note that the CPU doesn't switch page tables or stacks on an
interrupt.  For example, an interrupt in user mode will switch the
CPU to kernel mode and change the program counter to a
kernel-specific address, but still running with the user page table.
It is up to the kernel to complete the rest of the switch to kernel
mode: switch page tables, switch stacks, etc.  One reason that the
CPU provides minimal mechanisms for switching modes is that
provides flexibility to software; for example, for certain code path
it might not be necessary to switch page tables, and its performance
can be avoided.

To return from an interrupt to the original mode, RISC-V
provides the instructions \indexcode{mret} and
\indexcode{sret}. \lstinline{sret} sets the program counter to the
value stored in \lstinline{sepc} and the mode to previous mode stored
in \lstinline{sstatus}. \lstinline{mret} performs the corresponding
steps with m-registers.

Xv6 mainly uses 2 privilege levels: supervisor and user mode.  Xv6
programs the CPU to delegate all machine-mode interrupts to
supervisor mode. Timer interrupts, however, must be handled in machine
mode.  Thus, handling timer interrupts requires us to think about what
the kernel must do in all three modes. xv6 has a different plan for
each mode:

\begin{itemize}
  
\item Interrupts in machine mode.  Only timer interrupts can happen in
  machine mode and Xv6 does as little as possible: it resets the
  timer, sets a bit in the \lstinline{sip} register to indicate that
  there is an interrupt pending for the kernel, and returns from the
  interrupt to whatever mode it came from (kernel or user mode).
  Because there is an interrupt pending for supervisor mode, the
  CPU immediately will take it.

\item Interrupts in supervisor/kernel mode.  This case is simple. xv6
  is already in the kernel and there is no need to switch page
  tables. The kernel also has a valid stack, so Xv6 can immediately
  call a C function to handle the interrupt.

\item Interrupts in user mode. This case is challenging.  At the time
  of the interrupt, the user program is running with user-level page
  table and it will enter the kernel with that page table at a
  \textit{kernel-specified} virtual address. Furthermore, the kernel
  cannot use the stack of the user process to call a C function,
  because it may not be valid.  The user process may be malicious or
  contain an error that causes the user \texttt{sp} to contain an
  address that is not part of the process's user memory.

  To addresses these challenges, xv6 uses a \indextext{trampoline}
  page.  It maps a page with code to enter and exit the kernel in
  every process's address space and in the kernel address space at the
  address \indexcode{TRAMPOLINE} (as we saw in Figure~\ref{fig:as} and
  in Figure~\ref{fig:xv6_layout}).  This page is mapped at the same
  address in each address space.  This page contains the code that
  switches page tables, and because the page is mapped identically in
  each page table, the instruction right after the page table switch
  will translate identically.  The trampoline code for entering needs
  access to some area to save and restore CPU state, and therefore xv6
  also conveniently maps the process's trapframe (\lstinline{p->tf})
  in each process address space at \indexcode{TRAPFRAME}, right below
  \lstinline{TRAMPOLINE}.  The trampoline also switches from a user
  stack to a valid kernel stack.

\end{itemize}

A small hurdle in all three modes is that the interrupt handler often
needs some scratch space. When the interrupt handler starts it cannot
use any register because those contain values of the interrupted code
and they must be saved before the interrupt handler can use them. But,
to save them, it needs a register.  To handle this conundrum,
RISC-V provides a scratch register for machine and kernel
mode: \indexcode{mscratch} and \indexcode{sscratch}, respectively.
RISC-V also has an instruction to exchange the content
of a register with the content of the scratch register. After
executing this instruction, the register will contain the address of
the scratch area and the scratch register the content of the
register. The interrupt handler can now use the register to save other
registers in the scratch area.

This subsection has focused on interrupts, but the plan above works
also for exceptions and system calls: one can think off exceptions and
system calls as interrupts induced by software.  When the CPU
cannot execute an instruction in user space (e.g., dividing by zero),
the CPU switches to the interrupt handler in kernel mode.  To
switch to kernel mode for a system call, RISC-V provides
an instruction \indexcode{ecall}, which transfers control to the
interrupt handler.  To allow the interrupt handler to tell which
interrupt happened, RISC-V provides a register
\indexcode{scause}, which specifies the cause of the interrupt (e.g.,
timer interrupt, illegal instruction, system call).


\section{Code: machine mode interrupts}

By default all interrupts trap into machine mode.  xv6 programs
RISC-V to delegate all interrupts and exceptions to supervisor mode
\linerefs{kernel/start.c:/w_medeleg/,/w_mideleg/}.
Timer interrupts, however, must be received in machine
mode. Therefore, xv6 programs the hardware to receive interrupts in
machine mode: it stores into the \lstinline{mtvec} register the
address to jump to on an interrupt \lineref{kernel/start.c:/w_mtvec/}
and enables timer interrupts in the \lstinline{mie} register
\lineref{kernel/start.c:/w_mie/}.

When the CLINT interrupts the CPU, the CPU switches its
program counter to the value stored in \lstinline{mtvec} register,
which is \lstinline{machinevec}.  \lstinline{machinevec} is defined in
assembly \lineref{kernel/kernelvec.S:/machinevec/}.
\lstinline{machinevec} starts out by exchanging \lstinline{a0} and the
\lstinline{mscratch} register.  xv6 stored earlier in the
\lstinline{mscratch} register \lineref{kernel/start.c:/mscratch/} the
address \lstinline{mscratch0}, which points to an area of
\lstinline{uint64}s.  Entries 4 and 5 contain the address of the
core's \lstinline{CLINT_MTIME} and the new value
\linerefs{kernel/start.c:/scratch.4/,/scratch.5/}.
\lstinline{machinevec} saves registers \lstinline{a1} through
\lstinline{a3} in the scratch area so that it can use them for its own
computation.  Then, it reprograms \lstinline{CLINT_MTIMECMP} to
generate an interrupt in 10ms and sets the interrupt bit in the kernel
\lstinline{sip} register.  Setting this bit will cause the kernel to
receive an interrupt after \lstinline{machinevec} completes.
\lstinline{machinevec} completes by restoring the registers it used
and returning from machine mode \lineref{kernel/kernelvec.S:/mret/} to
the mode where it came from when it was interrupted.

The code that was interrupted cannot tell it was interrupted: the
CPU is in exactly the same state as before the interrupt,
because the \lstinline{machinevec} has restored all registers.  This
idea of saving the CPU state to run some code, and then
restoring the CPU state to resume the original code is a pattern
we will see a few more times.

\section{Code: kernel mode interrupts}

Handling interrupts in supervisor/kernel mode is similar to handling
interrupts in machine mode.  The main difference is that xv6 saves all
CPU state because it may want to switch to running a different
process.  In machine mode, xv6 just reprograms the CLINT and then
resumes the interrupted code.

The details are as follows. During start up xv6 stores the address of
\lstinline{kernelvec} in the trap vector register for supervisor mode
(the \lstinline{stvec} register)
\lineref{kernel/trap.c:/trapinithart/}.  \lstinline{kernelvec} saves
all registers on the current stack
\linerefs{kernel/kernelvec.S:/sd ra/,/sd t6/}
and then calls the C function \lstinline{kerneltrap}
\lineref{kernel/kernelvec.S:/call/}.  \lstinline{kerneltrap}
\lineref{kernel/trap.c:/kerneltrap/} performs a few sanity checks
(e.g., it checks that it was indeed interrupted in kernel mode) and
then yields the core so that another process can run.  We will discuss
the implementation of \lstinline{yield} in Chapter~\ref{CH:SCHED}.

At some point later, the scheduler may decide to resume the process
that was interrupted by the timer interrupt. The process returns from
\lstinline{yield}, perhaps running now on a different core.  It
restores \lstinline{sepc} register and \lstinline{sstatus} register.
\lstinline{kerneltrap} returns and restores all the saved registers
\linerefs{kernel/kernelvec.S:/ld ra/,/ld t6/}, adjusts the stack
pointer, and resumes at the addresses stored in \lstinline{sepc},
which is the address of the instruction that was interrupted.

\section{Code: user mode interrupts}

The function \lstinline{usertrapret}
\lineref{kernel/trap.c:/usertrapret/} sets up the trampoline to leave
and enter the kernel.  It starts out by disabling interrupts
\lineref{kernel/trap.c:/intr_off/}; xv6 is switching from taking
interrupts from \lstinline{kerneltrap} to \lstinline{usertrap} and
during this switch xv6 is not ready to take interrupts.

Next, \lstinline{usertrapret} sets the address where to enter the
kernel (by writing \lstinline{trampin} into \lstinline{stvec}) and it
sets up the values the trampoline needs when entering the kernel: the
kernel page table, the kernel stack of this process, the function to
call, and the hartid
\linerefs{kernel/trap.c:/kernel_satp.=.r_satp/,/r_tp/}.  We will see
how these values are used when entering the kernel.

Then, \lstinline{usertrapret} sets up values the trampoline needs to
exit the kernel and return to user space.  It will set the
previous mode in the \lstinline{sstatus} register to user mode (0) and
enable user interrupts in user mode, it will set the \lstinline{sepc}
register to the address the user program should be resumed at, and
will set the \lstinline{satp} register to this process's page table
\linerefs{kernel/trap.c:/x.=.r_sstatus/,/p->pagetable/}.
Finally, \lstinline{usertrapret} calls
the trampoline function \lstinline{trampout}, passing two arguments: the addresss
\lstinline{TRAPFRAME} and the value for \lstinline{satp}.

\lstinline{trampout} \lineref{kernel/trampoline.S:/trampout/} loads
\lstinline{satp} with the user page table.  After this instruction,
the CPU will translate virtual address with the user page table;
for example, the address of the next instruction will be translated
with the user page table.  This address will be translated in the same
way as with the kernel page table, because the mapping is the same in
both page tables.

To execute the remaining instructions, \lstinline{trampout} wants to
use register (\lstinline{a0}), which contains the first argument to
\lstinline{trampout}, \lstinline{TRAPFRAME}, the process's
trapframe. But, it must also restore \lstinline{a0} to the user
process's value for \lstinline{a0}, which is stored in the process's
trapframe \lstinline{p->tf}.  The kernel stores the to-be-restored value for
\lstinline{a0} in \lstinline{sscratch} and later restores
\lstinline{a0} from there \lineref{kernel/trampoline.S:/csrrw a0/}.

Now \lstinline{a0} is availabe, it uses it to restore all other CPU
registers from TRAPFRAME \linerefs{kernel/trampoline.S:/ld ra/,/ld t6/}.
Note that the user page table, which we just installed, maps
TRAPFRAME to \lstinline{p->tf}.  Thus, even though \lstinline{a0}
contains \lstinline{TRAPFRAME}, we are loading values from
\lstinline{p->tf}.  To make sure that user code cannot read/write
\lstinline{p->tf} through \lstinline{TRAPFRAME}, xv6 maps
\lstinline{TRAPFRAME} with the \lstinline{PTE_U} bit clear
\linerefs{kernel/proc.c:/TRAPFRAME/,/./}.

Then, \lstinline{trampout} restores \lstinline{a0} by switching the
values of \lstinline{sscratch} and \lstinline{a0}. After this
instruction, \lstinline{a0} contains the user process's \lstinline{a0}
and \lstinline{sscratch} contains \lstinline{p->tf}.  Having
\lstinline{p->tf} in \lstinline{sscratch} will be convenient when
entering: this is the address where xv6 will save registers.  Finally,
\lstinline{trampout} calls \lstinline{sret}, which returns to the
previous mode (user mode) with interrupts enabled and resuming at the
address in \lstinline{sepc}.  We are out of the kernel.

If a timer interrupt happens, the CPU will switch to kernel mode with
the program counter set to the value in \lstinline{stvec} (i.e.,
\lstinline{trampin}) and \lstinline{sepc} containing the address of
the instruction that was interrupted.  \lstinline{trampin} first saves
\lstinline{a0} and loads \lstinline{TRAPFRAME}, which maps to
\lstinline{p->tf}, into \lstinline{a0}
\lineref{kernel/trampoline.S:/csrrw a0/}.
Next, it saves all other
CPU registers into \lstinline{p->tf}
\linerefs{kernel/trampoline.S:/sd ra/,/sd t6/}.

Now all registers are saved, \lstinline{trampin} can
use \lstinline{t0} to save the original value of \lstinline{a0}.  It
switches to the kernel stack of the process, because the user stack
may not be valid.  It switches to the kernel page table; again, this
works out because trampoline page is mapped identically in the user
and kernel address space.  Finally, it calls \lstinline{usertrap} (its
address is in \lstinline{t0}).  We are back in the kernel with a
kernel page table and on a kernel stack, running C code.

One good property of the code above is that it also works for system
calls and exceptions.  If the user process wants to call a system call
or executes an illegal instruction (e.g., dividing by zero), xv6 uses
\lstinline{trampin} to enter the kernel, and \lstinline{usertrap} will
be invoked in all three cases (i.e., for system calls, exceptions, and
interrupts).

\section{Code: C trap handler}

The \lstinline{usertrap} function changes \lstinline{stvec} to the
address of \lstinline{kerneltrap} because xv6 is in kernel mode and
when xv6 enables interrupts in kernel mode, interrupts should go
through \lstinline{kerneltrap}; there is no need to switch page tables
and stacks. \lstinline{usertrap} next saves the \lstinline{sepc}
because that register contains the instruction where the process
should be resumed after completing the interrupt, exception, or system
call.  If the user process entered the kernel to execute a system
call, the register \lstinline{scause} contains 8.  In that case, xv6
adds 4 to \lstinline{p->tf->epc}, to skip the instruction that entered
the kernel (\lstinline{ecall}) so that on return to user space the
process will resume at the instruction after \lstinline{ecall}.
\lstinline{usertrap} then enables interrupts and calls
\lstinline{syscall}.

If an exception causes the kernel/user transition, then xv6 records
that the user process must be killed and will call \lstinline{exit}.
(We will look at how xv6 does this cleanup in Chapter~\ref{CH:SCHED}.)
Otherwise, the cause was an interrupt, which are dealt with by
\indexcode{devintr}.  For a timer
interrupt, \lstinline{devintr} just do two things: increment the ticks
variable \lineref{kernel/trap.c:/ticks\+\+/}, and call
\indexcode{wakeup}.  The latter, as
we will see in Chapter~\ref{CH:SCHED}, may make a process runnable.

After completing the system call or handling an interrupt, xv6 yields
the core to another process if the interrupt was a timer interrupt.
Otherwise, \lstinline{usertrap} exits the kernel and returns back to
user space using \lstinline{usertrapret}, which was explained above.
We have seen all the machinery to enter and leave the kernel.
 
\section{Code: Enabling/disabling interrupts}

A CPU can control if it wants to receive external and timer
interrupts through the \lstinline{SIE_SEIE}
\index{SIE_SEIE@\lstinline{SIE_SEIE}} flag and \lstinline{SIE_STIE}
\index{SIE_STIE@\lstinline{SIE_STIE}} flag in the \texttt{sie}
register, respectively.  The bootloader disables interrupts during
booting of the main cpu and the other other CPUs.  The scheduler on
each CPU enables interrupts \lineref{kernel/proc.c:/intr_on/}.  To
control that certain code fragments are not interrupted, xv6 disables
interrupts during these code fragments.  For example,
\lstinline{usertrapret} above clears interrupts
\lineref{kernel/trap.c:/intr_off/}.

\section{Code: System calls}

Chapter~\ref{CH:FIRST} ended with 
\indexcode{initcode.S}
invoking a system call.
Let's look at that again
\lineref{user/initcode.S:/SYS_exec/}.
The process pushed the arguments
for an 
\indexcode{exec}
call on the process's stack, and put the
system call number in
\texttt{a7}.
The system call numbers match the entries in the syscalls array,
a table of function pointers
\lineref{kernel/syscall.c:/syscalls/}.
The \lstinline{ecall} instruction
switches the CPU from user mode to kernel mode, and will
cause the kernel to call \lstinline{syscall}, as we saw above.

\indexcode{Syscall}
\lineref{kernel/syscall.c:/^syscall/} 
loads the system call number from the trapframe, which
contains the saved
\texttt{a7},
and indexes into the system call tables.
For the first system call, 
\texttt{a7}
contains the value 
\indexcode{SYS_exec}
\lineref{kernel/syscall.h:/SYS_exec/},
and
\lstinline{syscall}
will invoke the 
\lstinline{SYS_exec} 'th 
entry of the system call table, which corresponds to invoking
\lstinline{sys_exec}.

\lstinline{Syscall}
records the return value of the system call function in
\lstinline{p->tf->a0}.
When the system call returns to user space,
\lstinline{usertrapret}
will load the values
from
\indexcode{p->tf}
into the machine registers
and return to user space
using
\lstinline{sret}.
Thus, when 
\lstinline{exec}
returns, it will return in \lstinline{a0} the value
that the system call handler returned
\lineref{kernel/syscall.c:/a0 = syscalls/}.
System calls conventionally return negative numbers to indicate
errors, positive numbers for success.
If the system call number is invalid,
\indexcode{syscall}
prints an error and returns \-1.

\section{Code: System call arguments}

Later chapters will examine the implementation of
particular system calls.
This chapter is concerned with the mechanisms for system calls.
There is one bit of mechanism left: finding the system call arguments.
The helper functions
\lstinline{argint},
\lstinline{argaddr},
\lstinline{argptr},
\lstinline{argstr},
and
\lstinline{argfd}
retrieve the 
\textit{n} 'th 
system call
argument, as either an integer, pointer, a string, or a file descriptor.
\indexcode{argint}
and
\indexcode{argaddr}
use the function
\lstinline{fetcharg}
to locate the
\textit{n}'th 
argument. The C calling conventions specify that argument 0 is passed
through
\texttt{a0},
argument 1 through
\texttt{a1}, ...,
argument 6 through
\texttt{a6}.

\lstinline{argint} calls \indexcode{fetchint} to read the value at
that address from user memory and write it to \lstinline{*ip}.
\lstinline{fetchint} cannot simply cast the address to a pointer,
because the user and the kernel don't share the same page
table. Instead, \lstinline{fetchint} calls \lstinline{copyin} to copy
the value at the address from the process's address space to the
kernel address space.  Before calling \lstinline{copyin},
\lstinline{fetchint} verifies that the address lies in the process's
addres space: it checks that the address is below \indexcode{p->sz}.

\indexcode{copyin}
\lineref{kernel/vm.c:/^copyin/} copies \lstinline{len} bytes to
\lstinline{dst} from virtual address \lstinline{srcva} in the page
table \lstinline{pagetable}.  It walks the page table in software to
determine the physical address \lstinline{pa0} for \lstinline{srcva}.
Since the kernel maps virtual address one-to-one on physical
addresses and maps all physical addresses, the kernel can use
\lstinline{pa0} as a virtual address and copy the bytes at
\lstinline{pa0} using \lstinline{memmove}.

\indexcode{fetchaddr},
is like
\lstinline{fetchint},
but retrieves 64-bit value instead of a 32-bit int.

\indexcode{argptr}
fetches the
\textit{n}'th 
system call argument and checks that this argument is a valid
user-space pointer.

\indexcode{argstr} 
interprets the
\textit{n}'th 
argument as a pointer.  It ensures that the pointer points at a
NUL-terminated string and that the complete string is located below
the end of the user part of the address space.

Finally,
\indexcode{argfd}
\lineref{kernel/sysfile.c:/^argfd/}
uses
\lstinline{argint}
to retrieve a file descriptor number, checks if it is valid
file descriptor, and returns the corresponding
\lstinline{struct}
\lstinline{file}.

The system call implementations (for example, in sysproc.c and sysfile.c)
are typically wrappers: they decode the arguments using 
\lstinline{argint},
\lstinline{argaddr},
\lstinline{argptr}, 
and 
\lstinline{argstr}
and then call the real implementations.
In chapter~\ref{CH:MEM},
\lstinline{sys_exec}
uses these functions to get at its arguments.

\section{Drivers}

A
\indextext{driver}
is the code in an operating system that manages a particular device:
it tells the device hardware to perform operations,
configures the device to generate interrupts when done,
and handles the resulting interrupts.
Driver code can be tricky to write
because a driver executes concurrently with the device that it manages.  In
addition, the driver must understand the device's interface (e.g., which I/O
ports do what), and that interface can be complex and poorly documented.

The disk driver provides a good example.  The disk driver copies data
from and back to the disk.  Disk hardware traditionally presents the data on the
disk as a numbered sequence of 512-byte 
\textit{blocks} 
\index{block}
(also called 
\textit{sectors}): 
\index{sector}
sector 0 is the first 512 bytes, sector 1 is the next, and so on. The block size
that an operating system uses for its file system maybe different than the
sector size that a disk uses, but typically the block size is a multiple of the
sector size. To
represent a block xv6 has a structure
\lstinline{struct buf}
\lineref{kernel/buf.h:/^struct.buf/}.
The
data stored in this structure is often out of sync with the disk: it might have
not yet been read in from disk (the disk is working on it but hasn't returned
the sector's content yet), or it might have been updated but not yet written
out.  The driver must ensure that the rest of xv6 doesn't get confused when the
structure is out of sync with the disk.

\section{Code: Disk driver}

Xv6 has a disk driver for \indextext{virtio} disks. The
virtio standard is a standard for devices for kernels that run on a
virtual machine~\cite{virtio}.  It defines the interactions between a guest kernel
and the virtual machine monitor for virtual devices emulating network
devices, disk devices, etc.  Xv6 runs on \texttt{qemu} and
\texttt{qemu} supports a virtio disk device for RISC-V platforms.  The
base development board that xv6 targets (SiFive's HiFive) doesn't
provide a disk.

Xv6 represent file system blocks using
\indexcode{struct buf}
\lineref{kernel/buf.h:/^struct.buf/}.
\lstinline{BSIZE}
\lineref{kernel/fs.h:/BSIZE/}
is twice the virtio disk's sector size and thus
each buffer represents the contents of two sectors on a particular
disk device.  The
\lstinline{dev}
and
\lstinline{blockno}
fields give the device and block
number and the
\lstinline{data}
field is an in-memory copy of the disk sectors that correspond to block.
Operating systems often use
bigger blocks than 1,024 bytes to obtain higher disk throughput.

The
\lstinline{flags}
track the relationship between memory and disk:
the
\indexcode{B_VALID}
flag means that
\lstinline{data}
has been read in, and
the 
\indexcode{B_DIRTY} 
flag means that
\lstinline{data}
needs to be written out.

Like programming the CLINT, programming the virtio disk is done
through memory-mapped I/O.  The virtio disk is at address
\lstinline{0x10001000} in the kernel adddres space (see
Figure~\ref{fig:xv6_layout}).  The driver uses ordinary load and
stores instructions to interact with the device, but the addresses
have special meaning.  For example, reading from the address
\lstinline{0x10001000 + VIRTIO_MMIO_VERSION}
\lineref{kernel/virtio.h:/VERSION/} returns the version of the virtio
standard that the device supports
\lineref{kernel/virtio\_disk.c:/MMIO_VERSION/}.  The driver uses the
macro \lstinline{R} to read/write registers on the disk
\lineref{kernel/virtio\_disk.c:/R/}.

The kernel initializes the disk driver at boot time by calling
\indexcode{virtio_disk_init}
\lineref{kernel/virtio\_disk.c:/^virtio_disk_init/} from
\indexcode{main} \lineref{kernel/main.c:/virtio_disk_init/}.
\lstinline{virtio_disk_init} negotiates the simplest features with the
disk \linerefs{kernel/virtio\_disk.c:/features/,/=.features/} and sets
up one queue for disk requests \linerefs{kernel/virtio\_disk.c:/QUEUE_SEL/,/free\[/}.  The driver can have
\lstinline{NUM} outstanding requests to the disk.

Disk accesses typically take milliseconds, a long time for a
CPU.  Therefore, xv6 lets another process run on the CPU after a
disk I/O request and arranges to receive an interrupt when the disk
operation has completed.  
\indexcode{plic_init}, which is called from \lstinline{main}, programs
the \indextext{Platform Level Interrupt Controller (PLIC)}~\cite{riscv:priv} to
accept interrupts from the disk.  It enables interrupts for
\lstinline{VIRTIO0_IRQ} \lineref{kernel/plic.c:/^plicinit/}.  If the
disk generates an interrupt, \lstinline{devintr}
\lineref{kernel/trap.c:/^devintr/} will see that
\lstinline{VIRTIO0_IRQ} was enabled and invoke
\lstinline{virtio_disk_intr}.

After this setup, the disk is not used again until the buffer cache calls
\indexcode{virtio_disk_rw}
\lineref{kernel/virtio\_disk.c:/^virtio_disk_rw/},
which updates a locked buffer
as indicated by the flags.
If
\indexcode{B_DIRTY}
is set,
\lstinline{virtio_disk_rw}
writes the buffer
to the disk; if
\indexcode{B_VALID}
is not set,
\indexcode{virtio_disk_rw}
reads the buffer from the disk.
To add a request to the queue of requests, the driver must find 3
descriptors to describe the request.  The first descriptor
describes the operation (e.g., read/write).  The second descriptor
records the address of the data to be read or written. The third
descriptor is where the device will post the result status of the
request.  \lstinline{Virtio_Disk_Rw} starts by finding 3 free descriptors.  If
it cannot find 3 descriptors, it means that the queue of requests is
full, and it waits until a previously issued request completes, freeing
up descriptors.   If it finds 3 descriptors, the drivers fills them
in \linerefs{kernel/virtio\_disk.c:/buf0.type/,/.next.=.0/}.

The virtio disk supports \textit{Direct Memory Access
  (DMA)}\index{DMA}, which allows the device to read/write physical
memory.  The driver gives the device the physical address of the
buffer's data \lineref{kernel/virtio\_disk.c:/b->data/} and the device
copies directly to or from main memory.  DMA is faster and more
efficient than programmed I/O and is less taxing for the CPU's memory
caches.

Once the driver has filled out the descriptors, it can alert the disk
that a new quests is available.  It does so by writing to
\lstinline{avail}.  The disk monitors \lstinline{avail} and reads the
value in \lstinline{avail[1]} to find out how many new requests it
should process.  To make sure that writes to \lstinline{avail} are not
re-ordered, the driver inserts a memory barrier between them
\lineref{kernel/virtio\_disk.c:/sync_synchronize/}. This guarantees
that if the disk sees a new requests in \lstinline{avail[1]}, it will
also see the correct index in \lstinline{avail[2 + (avail[1]%NUM)]}.

Having posted the request to the disk,
\lstinline{virtio_disk_rw}
must wait for the result.  As discussed above,
polling does not make efficient use of the CPU.
Instead,
\indexcode{virtio_disk_rw}
yields the CPU for other processes by sleeping,
waiting for the interrupt handler to 
record in the buffer's flags that the operation is done
\linerefs{kernel/virtio\_disk.c:/while.*VALID/,/sleep/}.
While this process is sleeping,
xv6 will schedule other processes to keep the CPU busy.

Eventually, the disk will finish its operation and trigger an
interrupt, which will result in a call to
\indexcode{virtio_disk_intr}
to handle it
\lineref{kernel/trap.c:/virtio_disk_intr/}.
The disk posts the requests that it has completed in \lstinline{used}.
\lstinline{Virtio_Disk_Intr}
\lineref{kernel/virtio\_disk.c:/^virtio_disk_intr/}
reads \lstinline{used} to find the \lstinline{id} of
the request that has completed.
Then, \lstinline{virtio_disk_intr}
sets 
\indexcode{B_VALID},
clears
\indexcode{B_DIRTY} for the buffer in the request,
wakes up any process sleeping on the buffer
\linerefs{kernel/virtio\_disk.c:/info\[id\]\.b/,/wakeup/},
and frees up the descriptors that were used for request.
Freeing up a descriptor also wakes up processes
waiting for free descriptors.

The PLIC routes interrupts to the first CPU that claims it.
Thus, cores can process different disk interrupts concurrently.  In
some operating systems, this routing can get sophisticated.  For
example, a network driver might arrange to deliver interrupts for
packets of one network connection to the CPU that is managing
that connection, while interrupts for packets of another connection
are delivered to another CPU.  If some network connections are
short lived while others are long lived and the operating system wants
to keep all CPUs busy to achieve high throughput, this routing
becomes quite complex.

\section{Real world}

Supporting all the devices on a typical computer in its full glory is
much work, because there are many devices, the devices have many
features, and the protocol between device and driver can be complex
and badly documented.
In many operating systems, the drivers together account for more code
in the operating system than the core kernel.

Actual device drivers are far more complex than the disk driver in this chapter,
but the basic ideas are the same:
typically devices are slower than CPU, so the hardware uses
interrupts to notify the operating system of status changes.
Modern disk controllers typically
accept a 
\indextext{batch} 
of disk requests at a time and even reorder
them to make most efficient use of the disk arm.
When disks were simpler, operating systems often reordered the
request queue themselves.

Many operating systems have drivers for solid-state disks because they
provide much faster access to data.  But, although a solid-state disk
works very differently from a traditional mechanical disk, both
devices provide block-based interfaces and reading/writing blocks on a
solid-state disk is still more expensive than reading/writing RAM.
The virtio disk makes no distinction between a mechanical disk and
solid-state disk.

Other hardware is surprisingly similar to disks: network device
buffers hold packets, audio device buffers hold sound samples,
graphics card buffers hold video data and command sequences.
High-bandwidth graphics cards, and network cardsâ€”often use DMA
as the virtio disk does.

Some drivers dynamically switch between polling and interrupts, because using
interrupts can be expensive, but using polling can introduce delay until the
driver processes an event.  For example, a network driver that receives a
burst of packets may switch from interrupts to polling since it knows that more
packets must be processed and it is less expensive to process them using polling.
Once no more packets need to be processed, the driver may switch back to
interrupts, so that it will be alerted immediately when a new packet arrives.

If a program reads a file, the data for that file is copied twice.  First, it
is copied from the disk to kernel memory by the driver, and then later it is
copied from kernel space to user space by the 
\lstinline{read}
system call.  If the program then sends the data over the network, 
the data is copied twice more: from user space to kernel space and from
kernel space to the network device.  To support applications for which 
efficiency is important (e.g., serving popular images on the Web), operating systems
use special code paths to avoid copies.  As one example,
in real-world operating systems, 
buffers typically match the hardware page size, so that
read-only copies can be mapped into a process's address space
using the paging hardware, without any copying.

\section{Exercises}

\begin{enumerate}
  
\item Add a driver for an Ethernet card.

\end{enumerate}
